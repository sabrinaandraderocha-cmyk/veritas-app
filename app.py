import os
import re
import time
import difflib
from dataclasses import dataclass
from typing import Dict, List, Optional
from urllib.parse import urlparse

import streamlit as st

from veritas_utils import (
    extract_text_from_txt_bytes,
    extract_text_from_docx_bytes,
    extract_text_from_pdf_bytes,
    compute_matches,
    highlight_text,
)

# --- PDFs/DOCX (robusto para n√£o quebrar no Cloud) ---
from veritas_report import generate_pdf_report

try:
    from veritas_report import generate_web_pdf_report, generate_ai_pdf_report
except Exception:
    generate_web_pdf_report = None
    generate_ai_pdf_report = None

try:
    from veritas_report import generate_ai_docx_report
except Exception:
    generate_ai_docx_report = None


# =========================
# CONFIG GERAL
# =========================
APP_TITLE = "Veritas"

DISCL = (
    "O Veritas realiza an√°lise automatizada de similaridade textual. "
    "O resultado n√£o configura, por si s√≥, ju√≠zo definitivo sobre pl√°gio acad√™mico, "
    "o qual depende de avalia√ß√£o contextual e humana (cita√ß√µes, par√°frases, dom√≠nio p√∫blico, etc.)."
)

ETHICAL_NOTE = (
    "Similaridade n√£o √©, por si s√≥, falta √©tica. "
    "Trechos conceituais, metodologia, cita√ß√µes e f√≥rmulas recorrentes podem elevar a correspond√™ncia. "
    "Use o resultado como apoio de revis√£o, n√£o como veredito."
)

INTERNET_PRIVACY_NOTE = (
    "üîí **Privacidade**: ao usar o modo Internet, o Veritas envia **apenas trechos curtos** do seu texto "
    "(e n√£o o texto inteiro), para reduzir exposi√ß√£o. Mesmo assim, evite usar esse modo com textos sens√≠veis "
    "ou n√£o publicados se isso for um risco para voc√™."
)

AI_HEURISTIC_NOTE = (
    "‚ö†Ô∏è **Ressalva importante**\n\n"
    "Este m√≥dulo **n√£o comprova autoria** nem ‚Äúdetecta IA‚Äù com certeza. Ele apresenta **ind√≠cios heur√≠sticos** "
    "(padr√µes lingu√≠sticos e estat√≠sticos) que **podem ocorrer tanto em textos humanos quanto em textos gerados "
    "ou assistidos por IA**.\n\n"
    "Use o resultado **exclusivamente como apoio √† revis√£o**: fortalecer exemplos, fontes, precis√£o e marcas autorais."
)

# =========================
# PERFIS
# =========================
PROFILES = {
    "R√°pido (padr√£o)": {"chunk_words": 60, "stride_words": 25, "threshold": 0.75, "top_k_per_chunk": 1},
    "Rigoroso (c√≥pia literal)": {"chunk_words": 80, "stride_words": 35, "threshold": 0.82, "top_k_per_chunk": 1},
    "Sens√≠vel (par√°frase pr√≥xima)": {"chunk_words": 50, "stride_words": 20, "threshold": 0.66, "top_k_per_chunk": 1},
}

# =========================
# Leitura
# =========================
def _read_any(uploaded_file) -> str:
    name = uploaded_file.name.lower()
    b = uploaded_file.getvalue()
    if name.endswith(".txt"):
        return extract_text_from_txt_bytes(b)
    if name.endswith(".docx"):
        return extract_text_from_docx_bytes(b)
    if name.endswith(".pdf"):
        return extract_text_from_pdf_bytes(b)
    return extract_text_from_txt_bytes(b)

def _safe_words_count(text: str) -> int:
    return len(re.findall(r"\S+", text or ""))

def _band(global_sim: float):
    if global_sim < 0.15:
        return "üü¢ Similaridade esperada (baixa)", "Em geral, indica boa autonomia textual. Revise cita√ß√µes."
    if global_sim < 0.30:
        return "üü° Aten√ß√£o editorial (moderada)", "Pode refletir trechos comuns. Revise se√ß√µes sinalizadas."
    return "üü† Revis√£o cuidadosa (elevada)", "N√£o √© acusa√ß√£o. H√° sobreposi√ß√£o relevante: revise trechos e cita√ß√µes."

# =========================
# CSS leve
# =========================
def _inject_css():
    st.markdown(
        """
        <style>
          .muted { opacity: 0.75; }
          .pill {
            display:inline-block; padding: 0.18rem 0.55rem; border-radius: 999px;
            border: 1px solid rgba(49,51,63,0.18); margin-right: 0.35rem;
          }
          code { white-space: pre-wrap; }
        </style>
        """,
        unsafe_allow_html=True,
    )

# =========================
# INTERNET: SerpAPI
# =========================
def _get_serpapi_key() -> Optional[str]:
    key = None
    try:
        key = st.secrets.get("SERPAPI_KEY", None)
    except Exception:
        key = None
    if not key:
        key = os.getenv("SERPAPI_KEY")
    return key

def _split_words(text: str) -> List[str]:
    return re.findall(r"[A-Za-z√Ä-√ø0-9]+", (text or "").lower())

def build_chunks(text: str, chunk_words: int, stride_words: int, max_chunks: int = 12) -> List[str]:
    words = _split_words(text)
    if not words:
        return []
    chunks = []
    i = 0
    while i < len(words) and len(chunks) < max_chunks:
        chunk = words[i : i + chunk_words]
        if len(chunk) >= max(12, chunk_words // 2):
            chunks.append(" ".join(chunk))
        i += stride_words

    uniq = []
    seen = set()
    for c in chunks:
        k = c[:120]
        if k not in seen:
            uniq.append(c)
            seen.add(k)
    return uniq

def seq_similarity(a: str, b: str) -> float:
    a = (a or "").strip().lower()
    b = (b or "").strip().lower()
    if not a or not b:
        return 0.0
    return difflib.SequenceMatcher(None, a, b).ratio()

@dataclass
class WebHit:
    title: str
    link: str
    snippet: str
    score: float
    chunk: str

TRUST_BOOST_DOMAINS = [
    "scielo", "periodicos.capes", "pubmed", "ncbi.nlm.nih.gov",
    "doi.org", "springer", "wiley", "tandfonline", "elsevier", "sciencedirect",
    "jstor", "cambridge", "oxford", "sagepub", "nature.com", "science.org",
    "ieee.org", "acm.org"
]

PENALIZE_DOMAINS = [
    "brainly", "passeidireto", "scribd", "docsity", "monografias", "trabalhosprontos",
    "resumos", "blogspot", "wordpress", "medium.com", "reddit.com"
]

def _domain_of(url: str) -> str:
    try:
        netloc = urlparse(url).netloc.lower()
        return netloc.replace("www.", "")
    except Exception:
        return ""

def _domain_weight(domain: str) -> float:
    d = (domain or "").lower()
    if not d:
        return 1.0
    if d.endswith(".edu") or d.endswith(".gov"):
        return 1.25
    for t in TRUST_BOOST_DOMAINS:
        if t in d:
            return 1.18
    for p in PENALIZE_DOMAINS:
        if p in d:
            return 0.82
    return 1.0

def _snippet_quality(snippet: str) -> float:
    s = (snippet or "").strip()
    n = len(s)
    if n < 60:
        return 0.85
    if n < 120:
        return 0.95
    if n < 220:
        return 1.00
    return 1.06

def _clamp(x: float, lo: float, hi: float) -> float:
    return max(lo, min(hi, x))

def serpapi_search_chunk(chunk: str, serpapi_key: str, num_results: int = 5) -> List[Dict]:
    import requests  # precisa estar no requirements.txt

    q = f"\"{chunk}\"" if len(chunk) >= 80 else chunk
    params = {
        "engine": "google",
        "q": q,
        "api_key": serpapi_key,
        "num": num_results,
        "hl": "pt",
        "gl": "br",
    }

    r = requests.get("https://serpapi.com/search.json", params=params, timeout=25)
    r.raise_for_status()
    data = r.json()

    results = data.get("organic_results", []) or []
    cleaned = []
    for it in results[:num_results]:
        cleaned.append(
            {
                "title": (it.get("title") or "").strip(),
                "link": (it.get("link") or "").strip(),
                "snippet": (it.get("snippet") or "").strip(),
            }
        )
    return cleaned

def web_similarity_scan(
    text: str,
    serpapi_key: str,
    profile_params: dict,
    num_chunks: int = 10,
    num_results: int = 5,
    max_final_hits: int = 20,
) -> List[WebHit]:
    chunks = build_chunks(
        text,
        chunk_words=int(profile_params["chunk_words"]),
        stride_words=int(profile_params["stride_words"]),
        max_chunks=num_chunks,
    )

    raw_hits = []
    for c in chunks:
        try:
            results = serpapi_search_chunk(c, serpapi_key=serpapi_key, num_results=num_results)
        except Exception:
            continue

        for it in results:
            title = it.get("title", "") or ""
            link = it.get("link", "") or ""
            snippet = it.get("snippet", "") or ""

            combined = f"{title}\n{snippet}".strip()
            sim = seq_similarity(c, combined)

            domain = _domain_of(link)
            w_dom = _domain_weight(domain)
            w_snip = _snippet_quality(snippet)

            score_final = _clamp(sim * w_dom * w_snip, 0.0, 1.0)
            raw_hits.append(
                {"title": title, "link": link, "snippet": snippet, "domain": domain, "score": score_final, "chunk": c}
            )

    best_by_link = {}
    for h in raw_hits:
        link = h["link"]
        if not link:
            continue
        if link not in best_by_link or h["score"] > best_by_link[link]["score"]:
            best_by_link[link] = h

    deduped = list(best_by_link.values())
    deduped.sort(key=lambda x: x["score"], reverse=True)
    deduped = deduped[:max_final_hits]

    hits: List[WebHit] = []
    for h in deduped:
        title = h["title"]
        domain = h["domain"]
        if domain:
            title = f"{title}  ({domain})"
        hits.append(WebHit(title=title, link=h["link"], snippet=h["snippet"], score=h["score"], chunk=h["chunk"]))
    return hits


# =========================
# IA: heur√≠stica + explica√ß√£o humana
# =========================
AI_CONNECTORS = [
    "al√©m disso", "dessa forma", "nesse sentido", "por fim", "em suma", "portanto",
    "assim", "logo", "contudo", "entretanto", "todavia", "outrossim", "desse modo",
    "vale destacar", "√© importante destacar", "cabe ressaltar"
]
AI_VAGUE_WORDS = [
    "importante", "relevante", "significativo", "not√°vel", "essencial", "fundamental",
    "diversos", "v√°rios", "muitos", "alguns", "in√∫meros", "de certa forma",
    "em geral", "de modo geral", "de maneira geral"
]

def _sentences(text: str) -> List[str]:
    t = (text or "").strip()
    if not t:
        return []
    parts = re.split(r"(?<=[\.\!\?])\s+|\n+", t)
    return [p.strip() for p in parts if p.strip()]

def _tokens(text: str) -> List[str]:
    return re.findall(r"[A-Za-z√Ä-√ø0-9]+", (text or "").lower())

def _std(values: List[float]) -> float:
    if not values:
        return 0.0
    m = sum(values) / len(values)
    v = sum((x - m) ** 2 for x in values) / max(1, len(values))
    return v ** 0.5

def analyze_ai_indicia(text: str) -> Dict:
    t = (text or "").strip()
    toks = _tokens(t)
    sents = _sentences(t)

    word_count = len(toks)
    sent_word_lens = [len(_tokens(s)) for s in sents if len(_tokens(s)) > 0]

    unique = len(set(toks)) if toks else 0
    ttr = (unique / word_count) if word_count else 0.0

    mean_sent = (sum(sent_word_lens) / len(sent_word_lens)) if sent_word_lens else 0.0
    std_sent = _std([float(x) for x in sent_word_lens]) if sent_word_lens else 0.0
    cv_sent = (std_sent / mean_sent) if mean_sent > 0 else 0.0

    low = t.lower()
    conn_hits = sum(len(re.findall(rf"\b{re.escape(c)}\b", low)) for c in AI_CONNECTORS)
    vague_hits = sum(len(re.findall(rf"\b{re.escape(v)}\b", low)) for v in AI_VAGUE_WORDS)

    conn_per_1k = (conn_hits / max(1, word_count)) * 1000.0
    vague_per_1k = (vague_hits / max(1, word_count)) * 1000.0

    rep = 0
    for i in range(2, len(toks)):
        if toks[i] == toks[i-1] or toks[i] == toks[i-2]:
            rep += 1
    rep_per_1k = (rep / max(1, word_count)) * 1000.0

    score = 0.0
    if cv_sent > 0:
        score += _clamp((0.55 - cv_sent) / 0.55, 0.0, 1.0) * 30.0
    else:
        score += 10.0

    score += _clamp(conn_per_1k / 10.0, 0.0, 1.0) * 20.0
    score += _clamp(vague_per_1k / 18.0, 0.0, 1.0) * 20.0
    score += _clamp(rep_per_1k / 12.0, 0.0, 1.0) * 15.0

    if ttr > 0:
        score += _clamp((0.33 - ttr) / 0.33, 0.0, 1.0) * 15.0

    score = _clamp(score, 0.0, 100.0)

    if score < 33:
        band = ("üü¢ Baixa", "Poucos ind√≠cios de padroniza√ß√£o. Ainda assim, revise precis√£o, fontes e exemplos.")
    elif score < 66:
        band = ("üü° Moderada", "H√° sinais de padroniza√ß√£o. Reforce exemplos, especificidade e voz autoral.")
    else:
        band = ("üü† Elevada", "Sinais mais fortes de padroniza√ß√£o. Revise conectores, generalidades e detalhe emp√≠rico.")

    flagged_sentences = []
    for s in sents[:400]:
        sl = s.lower()
        c = sum(1 for x in AI_CONNECTORS if x in sl)
        v = sum(1 for x in AI_VAGUE_WORDS if x in sl)
        if (c + v) >= 2 and len(_tokens(s)) >= 10:
            flagged_sentences.append(s)

    return {
        "score": float(score),
        "band": band,
        "word_count": word_count,
        "sent_count": len(sents),
        "ttr": float(ttr),
        "mean_sent": float(mean_sent),
        "cv_sent": float(cv_sent),
        "conn_per_1k": float(conn_per_1k),
        "vague_per_1k": float(vague_per_1k),
        "rep_per_1k": float(rep_per_1k),
        "flagged_sentences": flagged_sentences[:12],
    }

def ai_friendly_explain(ai: dict) -> List[str]:
    score = float(ai.get("score", 0))
    cv = float(ai.get("cv_sent", 0))
    ttr = float(ai.get("ttr", 0))
    conn = float(ai.get("conn_per_1k", 0))
    vague = float(ai.get("vague_per_1k", 0))
    rep = float(ai.get("rep_per_1k", 0))

    lines = []

    if score < 33:
        lines.append("O texto mostra poucos ind√≠cios de padroniza√ß√£o. Se houver uso de IA, tende a ter sido leve ou bem revisado.")
    elif score < 66:
        lines.append("O texto tem sinais moderados de padroniza√ß√£o. Pode ser estilo acad√™mico ‚Äúmodelado‚Äù ou assist√™ncia de IA.")
    else:
        lines.append("O texto est√° bem padronizado. Isso pode ocorrer com IA/assist√™ncia intensa ou escrita muito ‚Äútemplate‚Äù. Vale revisar com calma.")

    actions = []
    if vague >= 2.5:
        actions.append("Troque trechos gen√©ricos por exemplos, dados, recortes temporais e cita√ß√µes.")
    if conn >= 6:
        actions.append("Varie conectores (ex.: ‚Äòal√©m disso‚Äô) e reescreva in√≠cios repetidos de par√°grafo.")
    if rep >= 10:
        actions.append("Revise repeti√ß√£o de palavras pr√≥ximas e use sin√¥nimos/explicita√ß√µes.")
    if ttr < 0.18:
        actions.append("Aumente a variedade de vocabul√°rio (sin√¥nimos, termos espec√≠ficos do tema).")
    if cv < 0.45:
        actions.append("Misture frases curtas e longas para dar ritmo e marca autoral.")

    if not actions:
        actions = ["Mantenha revis√£o de fontes, precis√£o conceitual e exemplos concretos (isso fortalece autoria)."]

    lines.append("O que fazer agora:")
    for a in actions[:3]:
        lines.append(f"‚Ä¢ {a}")

    return lines


# =========================
# State
# =========================
def _init_state():
    if "library" not in st.session_state:
        st.session_state["library"] = {}
    if "library_meta" not in st.session_state:
        st.session_state["library_meta"] = {}
    if "last_result" not in st.session_state:
        st.session_state["last_result"] = None
    if "profile" not in st.session_state:
        st.session_state["profile"] = "R√°pido (padr√£o)"
    if "internet_last" not in st.session_state:
        st.session_state["internet_last"] = None
    if "ai_last" not in st.session_state:
        st.session_state["ai_last"] = None


# =========================
# APP
# =========================
st.set_page_config(page_title=APP_TITLE, layout="wide")
_init_state()
_inject_css()

st.markdown(
    f"""
    <div>
      <h1>{APP_TITLE}</h1>
      <p class="muted">An√°lise de similaridade e integridade acad√™mica</p>
    </div>
    """,
    unsafe_allow_html=True,
)

with st.container(border=True):
    st.markdown("**Observa√ß√£o √©tica**")
    st.caption(DISCL)
    st.caption(ETHICAL_NOTE)

with st.sidebar:
    st.subheader("Modo de an√°lise")
    st.session_state["profile"] = st.selectbox(
        "Perfil",
        list(PROFILES.keys()),
        index=list(PROFILES.keys()).index(st.session_state["profile"]),
        key="profile_select",
    )
    st.caption(
        f"{st.session_state['profile']} ‚Üí "
        f"trecho {PROFILES[st.session_state['profile']]['chunk_words']} | "
        f"passo {PROFILES[st.session_state['profile']]['stride_words']} | "
        f"limiar {PROFILES[st.session_state['profile']]['threshold']}"
    )
    st.divider()
    st.subheader("Internet (opcional)")
    key = _get_serpapi_key()
    if key:
        st.success("SerpAPI key detectada ‚úÖ")
    else:
        st.caption("Modo Internet indispon√≠vel (SERPAPI_KEY n√£o configurada).")

tabs = st.tabs([
    "üß™ Biblioteca (privado)",
    "üåê Internet (externo)",
    "ü§ñ Ind√≠cios de Uso de IA (an√°lise heur√≠stica)",
    "üìö Biblioteca",
    "‚öôÔ∏è Sobre",
])

# =========================================================
# TAB 1: Biblioteca (privado)
# =========================================================
with tabs[0]:
    col1, col2 = st.columns([1.15, 0.85], gap="large")

    with col1:
        with st.container(border=True):
            st.subheader("Texto para an√°lise")
            mode = st.radio(
                "Como enviar o texto?",
                ["Colar texto", "Enviar arquivo"],
                horizontal=True,
                key="radio_biblioteca_envio",
            )
            query_name = "Texto colado"
            query_text = ""

            if mode == "Colar texto":
                query_text = st.text_area(
                    "Cole o texto do trabalho/artigo:",
                    height=280,
                    placeholder="Cole seu texto aqui..."
                )
            else:
                up = st.file_uploader(
                    "Envie um arquivo (.docx, .pdf, .txt)",
                    type=["docx", "pdf", "txt"],
                    key="upl_biblioteca",
                )
                if up is not None:
                    query_name = up.name
                    try:
                        query_text = _read_any(up)
                    except Exception as e:
                        st.error(f"N√£o consegui ler o arquivo. Erro: {e}")

            st.divider()
            run = st.button(
                "üîé Analisar (biblioteca)",
                type="primary",
                use_container_width=True,
                disabled=(not query_text or not st.session_state["library"]),
                key="btn_analisar_bib",
            )

    with col2:
        with st.container(border=True):
            st.subheader("Resumo")
            wc = _safe_words_count(query_text)
            st.markdown(f"<span class='pill'>üìÑ {wc} palavras</span>", unsafe_allow_html=True)
            st.write("Compara√ß√£o contra sua **Biblioteca Veritas** (modo privado).")

    if run:
        profile_params = PROFILES[st.session_state["profile"]]
        corpus = {
            n: t for n, t in st.session_state["library"].items()
            if not st.session_state["library_meta"].get(n, {}).get("exclude", False)
        }

        if not corpus:
            st.error("Sua biblioteca est√° vazia (ou tudo est√° exclu√≠do). V√° em **Biblioteca** e adicione fontes.")
        else:
            global_sim, matches = compute_matches(
                query_text=query_text,
                corpus_docs=corpus,
                chunk_words=int(profile_params["chunk_words"]),
                stride_words=int(profile_params["stride_words"]),
                top_k_per_chunk=int(profile_params["top_k_per_chunk"]),
                threshold=float(profile_params["threshold"]),
            )

            st.session_state["last_result"] = {
                "query_name": query_name,
                "query_text": query_text,
                "global_sim": float(global_sim),
                "matches": matches,
                "params": {"profile": st.session_state["profile"], **profile_params},
                "corpus_size": len(corpus),
                "ts": int(time.time()),
            }

    res = st.session_state.get("last_result")
    if res:
        st.divider()
        st.subheader("Resultado (Biblioteca)")

        global_sim = float(res.get("global_sim", 0.0))
        band_title, band_msg = _band(global_sim)
        st.info(f"**{band_title}** ‚Äî {band_msg}")

        left, right = st.columns([1, 1], gap="large")
        with left:
            matches = res.get("matches") or []
            if not matches:
                st.success("Nenhuma correspond√™ncia acima do limiar foi encontrada.")
            else:
                for i, m in enumerate(matches[:20], start=1):
                    st.markdown(f"**{i}.** `{m.source_doc}` ‚Äî **{m.score*100:.1f}%**")
                    st.caption("Trecho analisado")
                    st.write(m.query_chunk)
                    st.caption("Trecho fonte")
                    st.write(m.source_chunk)
                    st.divider()

        with right:
            highlighted = highlight_text(res["query_text"], res.get("matches") or [])
            st.text_area("Destaques (‚ü¶ ‚üß)", value=highlighted, height=330, key="ta_highlight_bib")

            pdf_path = os.path.join(os.getcwd(), f"Relatorio_Veritas_{res.get('ts', int(time.time()))}.pdf")
            generate_pdf_report(
                filepath=pdf_path,
                title="Relat√≥rio de An√°lise de Similaridade ‚Äì Veritas",
                query_name=res["query_name"],
                global_similarity=res["global_sim"],
                matches=res.get("matches") or [],
                params=res.get("params") or {},
                disclaimer=DISCL + "\n\n" + ETHICAL_NOTE,
            )
            with open(pdf_path, "rb") as f:
                st.download_button(
                    "‚¨áÔ∏è Baixar relat√≥rio (Biblioteca) em PDF",
                    data=f.read(),
                    file_name=os.path.basename(pdf_path),
                    mime="application/pdf",
                    use_container_width=True,
                    key="dl_pdf_bib",
                )

# =========================================================
# TAB 2: Internet (externo) ‚Äî agora com preset + avan√ßado opcional
# =========================================================
with tabs[1]:
    st.subheader("Similaridade na Internet (modo externo)")
    st.markdown(INTERNET_PRIVACY_NOTE)

    serp_key = _get_serpapi_key()
    if not serp_key:
        st.error("Modo Internet indispon√≠vel: configure `SERPAPI_KEY` nos secrets.")
    else:
        consent = st.checkbox(
            "‚úÖ Eu entendo e aceito que trechos do meu texto ser√£o enviados para busca na web.",
            value=False,
            key="internet_consent",
        )

        preset = st.selectbox(
            "N√≠vel de privacidade",
            ["Mais privado", "Equilibrado (recomendado)", "Mais completo"],
            index=1,
            help="Mais privado envia menos trechos. Mais completo aumenta chance de encontrar correspond√™ncias.",
            key="internet_priv_preset",
        )

        if preset == "Mais privado":
            num_chunks, num_results = 5, 4
        elif preset == "Mais completo":
            num_chunks, num_results = 14, 6
        else:
            num_chunks, num_results = 10, 5

        with st.expander("Op√ß√µes avan√ßadas (opcional)", expanded=False):
            num_chunks = st.slider("Trechos enviados", 3, 18, int(num_chunks), 1, key="internet_chunks_adv")
            num_results = st.slider("Resultados por trecho", 3, 10, int(num_results), 1, key="internet_results_adv")

        mode = st.radio(
            "Como enviar o texto?",
            ["Colar texto", "Enviar arquivo"],
            horizontal=True,
            key="radio_internet_envio",
        )

        query_name = "Texto colado"
        query_text = ""

        if mode == "Colar texto":
            query_text = st.text_area("Cole o texto para checar na internet:", height=220, key="internet_text")
        else:
            up = st.file_uploader(
                "Envie um arquivo (.docx, .pdf, .txt)",
                type=["docx", "pdf", "txt"],
                key="internet_uploader",
            )
            if up is not None:
                query_name = up.name
                try:
                    query_text = _read_any(up)
                except Exception as e:
                    st.error(f"N√£o consegui ler o arquivo. Erro: {e}")

        run_web = st.button(
            "üîé Buscar na internet",
            type="primary",
            use_container_width=True,
            disabled=(not consent or not query_text),
            key="btn_web",
        )

        if run_web:
            profile_params = PROFILES[st.session_state["profile"]]
            hits = web_similarity_scan(
                text=query_text,
                serpapi_key=serp_key,
                profile_params=profile_params,
                num_chunks=int(num_chunks),
                num_results=int(num_results),
                max_final_hits=20,
            )
            st.session_state["internet_last"] = {
                "query_name": query_name,
                "profile": st.session_state["profile"],
                "hits": hits,
                "ts": int(time.time()),
                "preset": preset,
                "num_chunks": int(num_chunks),
                "num_results": int(num_results),
            }

        webres = st.session_state.get("internet_last")
        if webres:
            hits: List[WebHit] = webres["hits"] or []
            if not hits:
                st.warning("N√£o encontrei resultados relevantes. Tente o modo ‚ÄúMais completo‚Äù.")
            else:
                top = hits[:10]
                global_web = sum(h.score for h in top) / max(1, len(top))
                st.metric("√çndice web (heur√≠stico)", f"{global_web*100:.1f}%")

                for i, h in enumerate(hits[:20], start=1):
                    st.markdown(f"**{i}. {h.title}** ‚Äî **{h.score*100:.1f}%**")
                    if h.link:
                        st.write(h.link)
                    if h.snippet:
                        st.write(h.snippet)
                    with st.expander("Trecho enviado (chunk)", expanded=False):
                        st.write(h.chunk)
                    st.divider()

                if generate_web_pdf_report is None:
                    st.warning("Relat√≥rio PDF (Internet) indispon√≠vel: atualize `veritas_report.py` no deploy.")
                else:
                    pdf_path_web = os.path.join(
                        os.getcwd(),
                        f"Relatorio_Veritas_Internet_{webres.get('ts', int(time.time()))}.pdf"
                    )
                    generate_web_pdf_report(
                        filepath=pdf_path_web,
                        title="Relat√≥rio de Similaridade ‚Äì Internet (Veritas)",
                        query_name=webres.get("query_name", "‚Äî"),
                        profile=webres.get("profile", "‚Äî"),
                        global_web_score=global_web,
                        hits=hits,
                        disclaimer=(
                            "Este relat√≥rio √© baseado em snippets e resultados p√∫blicos retornados por busca. "
                            "Ele n√£o comprova autoria ou pl√°gio; serve como apoio de revis√£o e checagem contextual.\n\n"
                            f"Configura√ß√£o usada: {webres.get('preset')} (trechos={webres.get('num_chunks')}, resultados/trecho={webres.get('num_results')})."
                        ),
                    )
                    with open(pdf_path_web, "rb") as f:
                        st.download_button(
                            "‚¨áÔ∏è Baixar relat√≥rio (Internet) em PDF",
                            data=f.read(),
                            file_name=os.path.basename(pdf_path_web),
                            mime="application/pdf",
                            use_container_width=True,
                            key="dl_pdf_web",
                        )

# =========================================================
# TAB 3: IA ‚Äî interpreta√ß√£o humana + m√©tricas opcionais + PDF + Word
# =========================================================
with tabs[2]:
    st.subheader("Ind√≠cios de Uso de IA (an√°lise heur√≠stica)")
    st.info(AI_HEURISTIC_NOTE)

    mode = st.radio(
        "Como enviar o texto?",
        ["Colar texto", "Enviar arquivo"],
        horizontal=True,
        key="radio_ai_envio",
    )
    query_name = "Texto colado"
    query_text = ""

    if mode == "Colar texto":
        query_text = st.text_area("Cole o texto para an√°lise heur√≠stica:", height=240, key="ai_text")
    else:
        up = st.file_uploader(
            "Envie um arquivo (.docx, .pdf, .txt)",
            type=["docx", "pdf", "txt"],
            key="ai_uploader",
        )
        if up is not None:
            query_name = up.name
            try:
                query_text = _read_any(up)
            except Exception as e:
                st.error(f"N√£o consegui ler o arquivo. Erro: {e}")

    run_ai = st.button(
        "ü§ñ Rodar an√°lise heur√≠stica",
        type="primary",
        use_container_width=True,
        disabled=(not query_text),
        key="btn_ai",
    )

    if run_ai:
        ai = analyze_ai_indicia(query_text)
        st.session_state["ai_last"] = {"query_name": query_name, "ts": int(time.time()), "ai": ai}

    aires = st.session_state.get("ai_last")
    if aires:
        ai = aires["ai"]
        band_title, band_msg = ai["band"]

        st.metric("√çndice heur√≠stico", f"{ai['score']:.0f}/100")
        st.info(f"**{band_title}** ‚Äî {band_msg}")

        st.markdown("### Interpreta√ß√£o (orientativa)")
        friendly_lines = ai_friendly_explain(ai)
        for line in friendly_lines:
            st.write(line)

        with st.expander("Ver m√©tricas t√©cnicas (opcional)", expanded=False):
            st.write(f"TTR (varia√ß√£o de vocabul√°rio): {ai['ttr']:.2f}")
            st.write(f"CV (varia√ß√£o do tamanho das frases): {ai['cv_sent']:.2f}")
            st.write(f"Conectores/1k: {ai['conn_per_1k']:.1f}")
            st.write(f"Vagueza/1k: {ai['vague_per_1k']:.1f}")
            st.write(f"Repeti√ß√£o/1k: {ai['rep_per_1k']:.1f}")

        if ai.get("flagged_sentences"):
            st.markdown("### Trechos sugeridos para revis√£o")
            for i, s in enumerate(ai["flagged_sentences"], start=1):
                st.write(f"**{i}.** {s}")

        # PDF IA (com interpreta√ß√£o junto no disclaimer)
        if generate_ai_pdf_report is None:
            st.warning("Relat√≥rio PDF (IA) indispon√≠vel: atualize `veritas_report.py` no deploy.")
        else:
            pdf_path_ai = os.path.join(
                os.getcwd(),
                f"Relatorio_Veritas_IA_{aires.get('ts', int(time.time()))}.pdf"
            )
            extra_interp = "\n".join(friendly_lines)
            disclaimer_ai_pdf = AI_HEURISTIC_NOTE + "\n\nInterpreta√ß√£o (orientativa):\n" + extra_interp

            generate_ai_pdf_report(
                filepath=pdf_path_ai,
                title="Relat√≥rio ‚Äì Ind√≠cios de Uso de IA (an√°lise heur√≠stica) ‚Äì Veritas",
                query_name=aires.get("query_name", "‚Äî"),
                ai_result=ai,
                disclaimer=disclaimer_ai_pdf,
            )
            with open(pdf_path_ai, "rb") as f:
                st.download_button(
                    "‚¨áÔ∏è Baixar relat√≥rio (IA ‚Äì heur√≠stico) em PDF",
                    data=f.read(),
                    file_name=os.path.basename(pdf_path_ai),
                    mime="application/pdf",
                    use_container_width=True,
                    key="dl_pdf_ai",
                )

        # WORD IA (DOCX)
        if generate_ai_docx_report is None:
            st.caption("Relat√≥rio Word (IA) indispon√≠vel: atualize `veritas_report.py` (generate_ai_docx_report).")
        else:
            docx_path = os.path.join(
                os.getcwd(),
                f"Relatorio_Veritas_IA_{aires.get('ts', int(time.time()))}.docx"
            )
            generate_ai_docx_report(
                filepath=docx_path,
                title="Relat√≥rio ‚Äì Ind√≠cios de Uso de IA (an√°lise heur√≠stica) ‚Äì Veritas",
                query_name=aires.get("query_name", "‚Äî"),
                ai_result=ai,
                disclaimer=AI_HEURISTIC_NOTE,
            )
            with open(docx_path, "rb") as f:
                st.download_button(
                    "‚¨áÔ∏è Baixar relat√≥rio (IA ‚Äì heur√≠stico) em Word",
                    data=f.read(),
                    file_name=os.path.basename(docx_path),
                    mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                    use_container_width=True,
                    key="dl_docx_ai",
                )

# =========================================================
# TAB 4: Biblioteca (upload)
# =========================================================
with tabs[3]:
    st.subheader("Biblioteca Veritas")

    up_lib = st.file_uploader(
        "Adicionar documentos (.docx, .pdf, .txt)",
        type=["docx", "pdf", "txt"],
        accept_multiple_files=True,
        key="upl_lib",
    )

    if up_lib:
        for f in up_lib:
            try:
                st.session_state["library"][f.name] = _read_any(f)
                st.session_state["library_meta"].setdefault(
                    f.name, {"tags": "", "category": "Refer√™ncia", "exclude": False}
                )
            except Exception as e:
                st.error(f"Falha ao ler {f.name}: {e}")
        st.success("Documentos adicionados.")

    st.divider()
    if st.session_state["library"]:
        for name in list(st.session_state["library"].keys()):
            meta = st.session_state["library_meta"].setdefault(
                name, {"tags": "", "category": "Refer√™ncia", "exclude": False}
            )
            c1, c2 = st.columns([0.8, 0.2])
            with c1:
                st.write(f"üìÑ {name}")
                meta["exclude"] = st.checkbox(
                    "Excluir",
                    value=bool(meta.get("exclude", False)),
                    key=f"exc_{name}",
                )
            with c2:
                if st.button("Remover", key=f"rm_{name}"):
                    del st.session_state["library"][name]
                    st.session_state["library_meta"].pop(name, None)
                    st.rerun()
    else:
        st.info("Ainda n√£o h√° documentos na biblioteca.")

# =========================================================
# TAB 5: Sobre
# =========================================================
with tabs[4]:
    st.subheader("Sobre o Veritas")
    st.markdown(
        """
- **Modo Biblioteca (privado):** compara apenas com os documentos que voc√™ adicionou.
- **Modo Internet (externo):** usa SerpAPI para buscar *trechos curtos* e comparar com snippets da web.
- **Ind√≠cios de Uso de IA (heur√≠stico):** mostra ind√≠cios de padroniza√ß√£o e recomenda√ß√µes (n√£o √© veredito).
- **Importante:** nenhum modo ‚Äúprova‚Äù pl√°gio ou IA; serve como apoio √† revis√£o e integridade acad√™mica.
        """
    )
    st.caption(DISCL)
    st.caption(ETHICAL_NOTE)
